---
date: "`r Sys.Date()`" 
output:
  html_document:
    theme: default
    highlight: espresso
    toc: yes
    toc_depth: 5
    toc_float:
      collapsed: yes
  pdf_document:
    toc: yes
    toc_depth: '5'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, results = TRUE, message = FALSE, warning = FALSE)
```


```{r results = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(ggplot2)
library(dplyr)
library (readr)
library(RMySQL)
library(gtrendsR)
library(stats)
library(DT)
library(rpart)
library(rpart.plot)
library(fastDummies)
library(randomForest)
library(e1071)
library(caret)
```


### Load data from RDS into R
### Initialize RDS Database connection
```{r}
user <-'admin'
password <-'!passKey607'
host<-'data607-project2.cbs1lxtno2zh.us-east-2.rds.amazonaws.com'

dbname <-'d_607_p3'
connection = dbConnect(MySQL(), user = user, password = password, dbname = dbname, host = host)
```

### Get data function
```{r}
get_data<- function(table_name) {
dataset <- tbl(connection,dbplyr::in_schema(dbname, table_name))
  return(dataset)
}
```

### The following is the query that returns all relevant records for project 3
```{r}

d607_p3_all_recs<-as.data.frame(get_data('view_d607_p3_all_recs'))

tibble(d607_p3_all_recs)

head(d607_p3_all_recs)

top_skills <- d607_p3_all_recs %>% 
                      select(skill_id, skill_desc) %>% 
                      group_by(skill_desc) %>% 
                      summarise(n = n()) %>%
                      arrange(desc(n))

```


```{r google-trends}

hl <-"en-US"
time <- "2010-01-01 2021-03-20"
geo <- "US"

# Google Trends for terms
python_trend <- gtrends(keyword = "Python", geo = geo, time = time, hl = hl)
sql_trend <- gtrends(keyword = "SQL", geo = geo, time = time, hl = hl)
ml_trend <- gtrends(keyword = "Machine Learning", geo = geo, time = time, hl = hl)
r_trend <- gtrends(keyword = "R", geo = geo, time = time, hl = hl)
hadoop_trend <- gtrends(keyword = "Hadoop", geo = geo, time = time, hl = hl)
spark_trend <- gtrends(keyword = "Spark", geo = geo, time = time, hl = hl)
java_trend <- gtrends(keyword = "Java", geo = geo, time = time, hl = hl)
tableau_trend <- gtrends(keyword = "Tableau", geo = geo, time = time, hl = hl)
datamining_trend <- gtrends(keyword = "Data Mining", geo = geo, time = time, hl = hl)
hive_trend <- gtrends(keyword = "Hive", geo = geo, time = time, hl = hl)
Sys.sleep(10)
sas_trend <- gtrends(keyword = "SAS", geo = geo, time = time, hl = hl)
bigtrend_trend <- gtrends(keyword = "Big Data", geo = geo, time = time, hl = hl)
aws_trend <- gtrends(keyword = "AWS", geo = geo, time = time, hl = hl)
scala_trend <- gtrends(keyword = "Scala", geo = geo, time = time, hl = hl)
nosql_trend <- gtrends(keyword = "NoSQL", geo = geo, time = time, hl = hl)
c_trend <- gtrends(keyword = "C/C++", geo = geo, time = time, hl = hl)
nlp_trend <- gtrends(keyword = "Natural Language Processing", geo = geo, time = time, hl = hl)
oracle_trend <- gtrends(keyword = "Oracle", geo = geo, time = time, hl = hl)
datawarehouse_trend <- gtrends(keyword = "Data Warehouse", geo = geo, time = time, hl = hl)
linux_trend <- gtrends(keyword = "Linux", geo = geo, time = time, hl = hl)
Sys.sleep(10)
ai_trend <- gtrends(keyword = "AI", geo = geo, time = time, hl = hl)
micrsql_trend <- gtrends(keyword = "Microsoft SQL Server", geo = geo, time = time, hl = hl)
tensorflow_trend <- gtrends(keyword = "TensorFlow", geo = geo, time = time, hl = hl)
kafka_trend <- gtrends(keyword = "Kafka", geo = geo, time = time, hl = hl)
azure_trend <- gtrends(keyword = "Azure", geo = geo, time = time, hl = hl)

```


```{r join-trends}

# Join trend data for terms rising in interest
python_trend$interest_over_time$ml_hits = ml_trend$interest_over_time$hits
python_trend$interest_over_time$spark_hits = spark_trend$interest_over_time$hits
python_trend$interest_over_time$tableau_hits = tableau_trend$interest_over_time$hits
python_trend$interest_over_time$kafka_hits = kafka_trend$interest_over_time$hits
python_trend$interest_over_time$azure_hits = azure_trend$interest_over_time$hits

# Join trend data for terms declining in interest
sql_trend$interest_over_time$java_hits = java_trend$interest_over_time$hits
sql_trend$interest_over_time$hadoop_hits = hadoop_trend$interest_over_time$hits
sql_trend$interest_over_time$datamining_hits = datamining_trend$interest_over_time$hits
sql_trend$interest_over_time$sas_hits = sas_trend$interest_over_time$hits
sql_trend$interest_over_time$oracle_hits = oracle_trend$interest_over_time$hits

# Join trend data for terms stable in interest
r_trend$interest_over_time$scala_hits = scala_trend$interest_over_time$hits
r_trend$interest_over_time$nosql_hits = nosql_trend$interest_over_time$hits
r_trend$interest_over_time$c_hits = c_trend$interest_over_time$hits
r_trend$interest_over_time$datawarehouse_hits = datawarehouse_trend$interest_over_time$hits

# Join trend data for terms that peaked in interest
bigtrend_trend$interest_over_time$tensorflow_hits = tensorflow_trend$interest_over_time$hits
bigtrend_trend$interest_over_time$hadoop_hits = hadoop_trend$interest_over_time$hits

```


```{r rising}

# Plot rising interest over time
ggplot() +
  geom_line(data=python_trend$interest_over_time, aes(y= hits, x= date, colour="a"), size=1 ) +
  geom_line(data=python_trend$interest_over_time, aes(y= ml_hits, x= date, colour="b"), size=1) +
  geom_line(data=python_trend$interest_over_time, aes(y= spark_hits, x= date, colour="c"), size=1) +
  geom_line(data=python_trend$interest_over_time, aes(y= tableau_hits, x= date, colour="d"), size=1) +
  geom_line(data=python_trend$interest_over_time, aes(y= azure_hits, x= date, colour="e"), size=1) +
  xlab("Date") + ylab("Interest over time") +
  scale_color_discrete(name = "Increasing Interest Over Time", 
                       labels = c("Python", "Machine Learning", "Spark", "Tableau", "Azure"))

```


```{r declining}

# Plot declining interest over time
ggplot() +
  geom_line(data=sql_trend$interest_over_time, aes(y=hits, x=date, color="a"), size=1) +
  geom_line(data=sql_trend$interest_over_time, aes(y=java_hits, x=date, color="b"), size=1) +
  geom_line(data=sql_trend$interest_over_time, aes(y=sas_hits, x=date, color="d"), size=1) +
  geom_line(data=sql_trend$interest_over_time, aes(y=oracle_hits, x=date, color="e"), size=1) +
  xlab("Date") + ylab("Interest over time") +
  scale_color_discrete(name = "Decreasing Interest Over Time", 
                       labels = c("SQL", "Java", "SAS", "Oracle"))

```


```{r stable}

# Plot stable interest over time
ggplot() +
  geom_line(data=r_trend$interest_over_time, aes(y=hits, x=date, color="a"), size=1) +
  geom_line(data=r_trend$interest_over_time, aes(y=scala_hits, x=date, color="b"), size=1) +
  geom_line(data=r_trend$interest_over_time, aes(y=nosql_hits, x=date, color="c"), size=1) +
  geom_line(data=r_trend$interest_over_time, aes(y=c_hits, x=date, color="d"), size=1) +
  geom_line(data=r_trend$interest_over_time, aes(y=datawarehouse_hits, x=date, color="e"), size=1) +
  xlab("Date") + ylab("Interest over time") +
  scale_color_discrete(name = "Stable Interest Over Time", 
                       labels = c("R", "Scala", "NoSQL", "C", "Data Warehouse"))

```


```{r peaked}

# Plot peaked interest over time
ggplot() +
  geom_line(data=bigtrend_trend$interest_over_time, aes(y=hits, x=date, color="a"), size=1) +
  geom_line(data=bigtrend_trend$interest_over_time, aes(y=as.numeric(tensorflow_hits), 
                                                        x=date, color="b"), size=1) +
  geom_line(data=bigtrend_trend$interest_over_time, aes(y=hadoop_hits, x=date, color="c"), size=1) +
  xlab("Date") + ylab("Interest over time") +
  scale_color_discrete(name = "Peak Popularity", labels = c("Big Data", "Tensorflow", "Hadoop"))

```


```{r prep-data-for-modeling}

# Include skills with at least 100 observations
skills_hundred_obs <- top_skills %>% filter(n >= 100)
df_jobs_skills_hundred_obs <- d607_p3_all_recs %>% filter(skill_desc %in% skills_hundred_obs$skill_desc)

# Create dummy variables for skills with at least 100 observations
df_jobs_skills_hundred_obs_dummies <- dummy_cols(df_jobs_skills_hundred_obs, 
                                      select_columns = c("skill_desc"), remove_first_dummy = TRUE)

# Remove unneeded columns
df_jobs_skills_hundred_obs_dummies <- df_jobs_skills_hundred_obs_dummies %>% 
                                      select(-(c("job_title", "company", "no_of_emp", 
                                         "revenue_range", "industry", 
                                         "postal_code", "skill_id", "skill_desc")))

# Group by job_id and for each column use max value (0 or 1) as aggregate value
df_jobs_skills_hundred_obs_dummies <- df_jobs_skills_hundred_obs_dummies %>% 
                                                      group_by(job_id) %>% 
                                                      summarise_each(list(max))

# Find top locations by observation count
top_locations <- d607_p3_all_recs %>% select(skill_id, postal_code) %>% 
                                      group_by(postal_code) %>% 
                                      summarise(n = n()) %>%
                                      arrange(desc(n))

# Include locations with at least 100 observations
locations_hundred_obs <- top_locations %>% filter(n >= 100)
df_jobs_locs_hundred_obs <- d607_p3_all_recs %>% 
                                 filter(postal_code %in% locations_hundred_obs$postal_code)

# Create dummy variables for locations with at least 100 observations
df_jobs_locs_hundred_obs_dummies <- dummy_cols(df_jobs_locs_hundred_obs, 
                                      select_columns = c("postal_code"), remove_first_dummy = TRUE)

# Remove unneeded columns
df_jobs_locs_hundred_obs_dummies <- df_jobs_locs_hundred_obs_dummies %>% 
                                    select(-(c("job_title", "company", "no_of_emp", 
                                         "salary_range", "revenue_range", "industry", 
                                         "postal_code", "skill_id", "skill_desc")))

# Group by job_id and for each column use max value of one of the rows as aggregate value
df_jobs_locs_hundred_obs_dummies <- df_jobs_locs_hundred_obs_dummies %>% 
                                                      group_by(job_id) %>% 
                                                      summarise_each(list(max))

# Join skills and locations
df_model <- df_jobs_skills_hundred_obs_dummies %>% inner_join(df_jobs_locs_hundred_obs_dummies)

# Rename problematic column names
df_model <- df_model %>% rename(skill_desc_C = "skill_desc_C/C++",
                                        skill_desc_C_Sharp = "skill_desc_C#",
                                        skill_desc_CICD = "skill_desc_CI/CD",
                                        skill_desc_TSSCIClearance = "skill_desc_TS/SCIClearance")

```


```{r train-test-split}

# Create train test split
train <- df_model %>% dplyr::sample_frac(.75)
test  <- dplyr::anti_join(df_model, train, by = 'job_id')

```


```{r class-sizes}

# Reorder target variable levels
positions <- c("<80000", "80000-99999", "100000-119999", "120000-139999", "140000-159999", ">160000")
df_model$salary_range <- factor(df_model$salary_range, levels=positions)

# To get an idea of minimum accuracy
# Review the distribution / class sizes of the target variable by percentage
ggplot(df_model, aes(x = salary_range)) +  
  geom_bar(aes(y = (..count..)/sum(..count..)), fill="turquoise3") + 
  ggtitle("Salary Range by Percentage") + xlab("Percentage") + ylab("Salary Range")

```


```{r classification-tree}

# Fit model training data
model_ct <- rpart(salary_range ~ . - job_id, method="class", data=train)

# Plot classification tree
rpart.plot(model_ct, main="Classification Tree for Salary Range")

# Make predictions on test data
preds_ct <- predict(model_ct, as.data.frame(test), type = "class")

# Review Confusion Matrix
confusionMatrix(preds_ct, as.factor(test$salary_range))

```


```{r random-forest}

# Fit Top Skills and Locations Random Forest Model
model_rf <- randomForest(as.factor(salary_range) ~ . - job_id, method="class",  data=train)

# Make predictions on test data
preds_rf <- predict(model_rf, test)

# Review Confusion Matrix
confusionMatrix(preds_rf, as.factor(test$salary_range))

```


```{r naive-bayes}

# Fit Naive Bayes model
model_nb <- naiveBayes(as.factor(salary_range)  ~ . - job_id, data = train)

# Make predictions on test data
preds_nb <- predict(model_nb, as.data.frame(test))

# Review Confusion Matrix
confusionMatrix(preds_nb, as.factor(test$salary_range))

```

