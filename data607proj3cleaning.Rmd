---
title: "data607proj3"
author: "Richard"
date: "3/18/2021"
output: html_document
---
# packages
```{r}
library(tidyverse)
library(wordcloud)
```

# loading data

```{r}
url = 'https://raw.githubusercontent.com/tagensingh/SPS-DATA607-P-3/main/indeed_job_dataset.csv'
data = read.csv(url)
dim(data)
```

# functions used

```{r}
#function that transforms "python list" string into R vector
clean_string = function(string){
list = str_replace(string,"\\[","") %>%
  str_replace("\\]","") %>%
  str_replace_all("\\'","") %>%
  str_split(",")
return (trimws(list[[1]]))
}
```


# cleaning data


#### job title

```{r}
# subset job titles
job_title = unique(data[c('Job_Title','Job_Type')])
head(job_title)
```

#### company

```{r}
# subset companies
companies = unique(data[c('Company','No_of_Reviews','No_of_Stars','Company_Revenue','Company_Employees','Company_Industry')])
# drop nulls
companies = drop_na(companies)
head(companies)
```

#### jobs

```{r}
# subset jobs
jobs = data[c('X','Job_Title','Company','Queried_Salary','Date_Since_Posted','Location','Link')]
head(jobs)
```

#### skills

```{r}
data = data[data$No_of_Skills>0,]
x = data$X
strings = data$Skill
count = c()
X = c()
skill_list = c()

# create vector count
for (string in strings){
  count = c(count,length(clean_string(string)))
}

#create skills dataframe
for (i in seq(length(count))){
  X = c(X,rep(x[i],count[i]))
  skill_list = c(skill_list,clean_string(strings[i]))
}

skills = data.frame(X,skill_list)
head(skills)
```

# download tables as csv

```{r}
#write.csv(job_title,"job_title.csv",row.names=FALSE)
#write.csv(companies,"companies.csv",row.names=FALSE)
#write.csv(jobs,"jobs.csv",row.names=FALSE)
#write.csv(skills,"skills.csv",row.names=FALSE)
```

# EDA

#### These are the top 10 jobs that companies are looking for. Most notably, over 60% of jobs are looking for candidates with python knowledge and over 50% of jobs require SQL. R and Machine learning are also incredibly saught after with nearly 40% of the jobs looking for candidates with those skills 

```{r}
# most popular skills
num_jobs = dim(jobs)[1]
skills %>%
  group_by_at('skill_list')%>%
  summarise(percent = n()/num_jobs)%>%
  arrange(desc(percent))%>%
  head(10)%>%
  ggplot(aes(reorder(skill_list,percent),percent))+geom_bar(stat = 'identity',fill = '#A1B8A1')+coord_flip()+labs(x = 'Skills',title = "Most Desired Data Science Skills in the U.S.")

```

#### wordcloud that shows popularity of skills based on size of the word 

```{r}
skill_count = skills %>%
  group_by(skill_list)%>%
  summarise(frequency = n())%>%
  arrange(desc(frequency))
  

wordcloud(skill_count$skill_list,skill_count$frequency,min.freq = 5,max.words=150, random.order=FALSE, rot.per=0.1,            colors=brewer.pal(8, "Dark2"))


```

#### Just looking at jobs in NY, SQL and Python are desired for most roles (over 50%) with R and Machine Learning being necessary for nearly 40%. These results are very similar to our results for the entire United States

```{r}
ny = jobs[jobs["Location"] == "NY",]
ny_skills = merge(ny,skills,on = 'X')
num_jobs_ny = dim(ny)[1]

ny_skills %>% 
  group_by(skill_list)%>%
  summarise(percent = n()/num_jobs_ny)%>%
  arrange(desc(percent))%>%
  head(10)%>%
  ggplot(aes(reorder(skill_list,percent),percent))+geom_bar(stat = 'identity', fill = '#C4A9C4')+coord_flip()+labs(x = 'Skills',title = "Most Desired Data Science Skills Skills in NY")
```


